{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn as nl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the rewiring probabilities of two groups\n",
    "pws_1 = 0.9\n",
    "pws_2 = 0.67\n",
    "num_trn = 1800\n",
    "num_val = 200\n",
    "num_node = 50\n",
    "avg_degree = 10\n",
    "\n",
    "#Computation graph params\n",
    "num_layer = 2\n",
    "input_size = 1275\n",
    "output_size = 1\n",
    "num_hidden_units = 500\n",
    "dp_rate = 0.5\n",
    "wd_rate = 0.001\n",
    "l_r = 0.00015\n",
    "num_epoch = 100\n",
    "batch_size = 10\n",
    "\n",
    "np.random.seed((int)(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate Watts-Strogatz Networks\n",
    "def w_s_network(num_node, pws, avg_degree):\n",
    "    adjacency_matrix = np.zeros((num_node, num_node))\n",
    "    \n",
    "    #Connect each node to avg_degree/2 nearest neighbour (eg. node 0 will connect to node 1 and node num_node-1)\n",
    "    half_k = (int)(avg_degree/2)\n",
    "    \n",
    "    for node_i in range(num_node):\n",
    "        start = node_i - half_k\n",
    "        index = start - 1\n",
    "        for j in range(avg_degree):\n",
    "            index = index + 1\n",
    "            if index == node_i:\n",
    "                index = index + 1\n",
    "            if index >= num_node:\n",
    "                index = index - num_node\n",
    "            if adjacency_matrix[node_i, index] != 1:\n",
    "                adjacency_matrix[node_i, index] = 1\n",
    "    \n",
    "    #Rewiring connections\n",
    "    prob_matrix = np.random.rand(num_node, num_node)\n",
    "    for node_i in range(num_node):\n",
    "        prob_matrix[node_i, node_i] = 1\n",
    "    \n",
    "    for node_i in range(num_node):\n",
    "        for node_j in range(num_node):\n",
    "            if adjacency_matrix[node_i, node_j] == 1 and prob_matrix[node_i, node_j] < pws:\n",
    "                adjacency_matrix[node_i, node_j] = 0\n",
    "                adjacency_matrix[node_j, node_i] = 0\n",
    "                \n",
    "                rewired = False\n",
    "                while rewired == False:\n",
    "                    index_i = np.random.randint(num_node)\n",
    "                    index_j = np.random.randint(num_node)\n",
    "                    if index_i != index_j and adjacency_matrix[index_i, index_j] == 0:\n",
    "                        adjacency_matrix[index_i, index_j] = 1\n",
    "                        adjacency_matrix[index_j, index_i] = 1\n",
    "                        rewired = True\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate 1000 samples (800 trn and 200 val)\n",
    "#Each sample has 50 nodes and each node has average degree of 8\n",
    "data = {}\n",
    "\n",
    "trn_x = np.zeros((num_trn, num_node * num_node))\n",
    "trn_y = np.zeros((num_trn, 1))\n",
    "val_x = np.zeros((num_val, num_node * num_node))\n",
    "val_y = np.zeros((num_val, 1))\n",
    "\n",
    "for i in range(num_trn):\n",
    "    group = np.random.randint(2)\n",
    "    if group == 0:\n",
    "        sample_x = w_s_network(50, pws_1, avg_degree)\n",
    "        sample_y = 0\n",
    "    \n",
    "    if group == 1:\n",
    "        sample_x = w_s_network(50, pws_2, avg_degree)\n",
    "        sample_y = 1\n",
    "    \n",
    "    trn_x[i, :] = sample_x.reshape(1, -1)\n",
    "    trn_y[i] = sample_y\n",
    "\n",
    "for i in range(num_val):\n",
    "    group = np.random.randint(2)\n",
    "    if group == 0:\n",
    "        sample_x = w_s_network(50, pws_1, avg_degree)\n",
    "        sample_y = 0\n",
    "    \n",
    "    if group == 1:\n",
    "        sample_x = w_s_network(50, pws_2, avg_degree)\n",
    "        sample_y = 1\n",
    "    \n",
    "    val_x[i, :] = sample_x.reshape(1, -1)\n",
    "    val_y[i] = sample_y\n",
    "\n",
    "data['trn_x'] = trn_x\n",
    "data['trn_y'] = trn_y\n",
    "data['val_x'] = val_x\n",
    "data['val_y'] = val_y\n",
    "\n",
    "pkl.dump(data, open(os.path.join('.', 'data.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate each sample to save only the bottom left part\n",
    "truncated_vector_size = (int)((1+num_node)*num_node/2)\n",
    "truncated_data = {}\n",
    "truncated_data['trn_y'] = data['trn_y']\n",
    "truncated_data['val_y'] = data['val_y']\n",
    "truncated_data['trn_x'] = np.zeros((num_trn, truncated_vector_size))\n",
    "truncated_data['val_x'] = np.zeros((num_val, truncated_vector_size))\n",
    "for i in range(num_trn):\n",
    "    truncated_trn_x = np.zeros((1, truncated_vector_size))\n",
    "    start = 0\n",
    "    for j in range(num_node):\n",
    "        start = start + j\n",
    "        truncated_trn_x[0, start:start+j] = data['trn_x'][i, j*num_node : j*num_node + j]\n",
    "    truncated_data['trn_x'][i, ] = truncated_trn_x\n",
    "\n",
    "for i in range(num_val):\n",
    "    truncated_val_x = np.zeros((1, truncated_vector_size))\n",
    "    start = 0\n",
    "    for j in range(num_node):\n",
    "        start = start + j\n",
    "        truncated_val_x[0, start:start+j] = data['val_x'][i, j*num_node : j*num_node + j]\n",
    "    truncated_data['val_x'][i, ] = truncated_val_x\n",
    "    \n",
    "pkl.dump(truncated_data, open(os.path.join('.', 'truncated_data.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.]\n",
      "[1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x268837b83c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x268836d1ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_t = pkl.load(open( \"data.p\", \"rb\" ))\n",
    "truncated_data_t = pkl.load(open( \"truncated_data.p\", \"rb\" ))\n",
    "\n",
    "trn_sample = data_t['trn_x'][55, ]\n",
    "trn_sample_t = truncated_data_t['trn_x'][55, ]\n",
    "print(trn_sample)\n",
    "print(trn_sample_t)\n",
    "print(data_t['trn_y'][55])\n",
    "val_sample = data_t['val_x'][114, ]\n",
    "print(data_t['val_y'][114])\n",
    "\n",
    "#m = m.reshape(50, -1)\n",
    "\n",
    "plt.imshow(trn_sample.reshape(50, 50), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(val_sample.reshape(50, 50), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(cur_layer, out_size, dp_rate, is_input):\n",
    "    \n",
    "    _, cur_size = [item.value for item in cur_layer.shape]\n",
    "        \n",
    "    var = 2.0 / (cur_size + out_size)\n",
    "\n",
    "    stddev = math.sqrt(var)\n",
    "\n",
    "    print(\"steddev is\", stddev)\n",
    "    print(out_size)\n",
    "\n",
    "    with tf.name_scope('fc'):\n",
    "        W_fc = tf.Variable(tf.truncated_normal(shape=[cur_size,out_size], stddev=stddev))\n",
    "        b_fc = tf.Variable(tf.constant(0.0, shape=[out_size]))\n",
    "        if is_input is False: \n",
    "            cur_layer = tf.nn.dropout(cur_layer, 1 - dp_rate)\n",
    "        out_layer = tf.matmul(cur_layer, W_fc) + b_fc\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "def activation(cur_layer, is_output=False):\n",
    "    if is_output is True:\n",
    "        return tf.nn.sigmoid(cur_layer)\n",
    "    return tf.nn.leaky_relu(cur_layer)\n",
    "\n",
    "def loss_function(yhat, y):\n",
    "    '''\n",
    "    '''\n",
    "    # get the weight decay terms\n",
    "    with tf.name_scope('wd_term'):\n",
    "        wght_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='fc')\n",
    "        wd_term = wd_rate * tf.stack([tf.nn.l2_loss(i) for i in wght_params])\n",
    "\n",
    "    with tf.name_scope('crit_loss'):\n",
    "            crit = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=yhat)) + tf.reduce_sum(wd_term)\n",
    "\n",
    "    return crit\n",
    "\n",
    "def generate_dataset(x, y, shuffle=True):\n",
    "    with tf.name_scope('dataset'):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        if shuffle: \n",
    "            dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(?, 1275)\n",
      "steddev is 0.033567254331867566\n",
      "500\n",
      "1\n",
      "(?, 500)\n",
      "steddev is 0.06318240236065634\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Build the actual graph\n",
    "tf.reset_default_graph()\n",
    "x_place = tf.placeholder('float', [None, input_size], name='input_x')\n",
    "y_place = tf.placeholder('float', [None,  output_size], name='target_y')\n",
    "dp_place = tf.placeholder(tf.float32, name='dropout_value')\n",
    "\n",
    "cur_layer = x_place\n",
    "\n",
    "for i in range(num_layer):\n",
    "    print(i)\n",
    "    if i != (num_layer - 1):\n",
    "        print(cur_layer.shape, flush=True)\n",
    "        cur_layer = forward_prop(cur_layer, num_hidden_units, dp_place, i == 0)\n",
    "        cur_layer = activation(cur_layer)\n",
    "    else:\n",
    "        print(cur_layer.shape, flush=True)\n",
    "        cur_layer = forward_prop(cur_layer, output_size, dp_place, i == 0)\n",
    "        yhat_op = activation(cur_layer, True)\n",
    "\n",
    "#Cross entropy loss\n",
    "loss_op = loss_function(yhat_op, y_place)\n",
    "tf.summary.scalar('Cross_Entropy_Loss', loss_op)\n",
    "\n",
    "#adam\n",
    "optimizer = tf.train.AdamOptimizer(l_r)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred = tf.equal(y_place, tf.round(yhat_op)) \n",
    "acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('Accuracy', acc_op)\n",
    "\n",
    "#Iterators\n",
    "truncated_data = pkl.load(open( \"truncated_data.p\", \"rb\" ))\n",
    "trn_it = generate_dataset(truncated_data['trn_x'], truncated_data['trn_y'], shuffle=True)\n",
    "val_it = generate_dataset(truncated_data['val_x'], truncated_data['val_y'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00000 - TRN: loss 0.91 VAL: loss 0.84\n",
      "\n",
      "Ep.00000 - TRN: acc 0.61 VAL: acc 0.88\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00001 - TRN: loss 0.77 VAL: loss 0.76\n",
      "\n",
      "Ep.00001 - TRN: acc 0.94 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00002 - TRN: loss 0.72 VAL: loss 0.73\n",
      "\n",
      "Ep.00002 - TRN: acc 0.97 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00003 - TRN: loss 0.69 VAL: loss 0.70\n",
      "\n",
      "Ep.00003 - TRN: acc 0.98 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00004 - TRN: loss 0.66 VAL: loss 0.69\n",
      "\n",
      "Ep.00004 - TRN: acc 0.99 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00005 - TRN: loss 0.64 VAL: loss 0.67\n",
      "\n",
      "Ep.00005 - TRN: acc 0.99 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00006 - TRN: loss 0.63 VAL: loss 0.65\n",
      "\n",
      "Ep.00006 - TRN: acc 0.99 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00007 - TRN: loss 0.61 VAL: loss 0.64\n",
      "\n",
      "Ep.00007 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00008 - TRN: loss 0.60 VAL: loss 0.63\n",
      "\n",
      "Ep.00008 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00009 - TRN: loss 0.59 VAL: loss 0.62\n",
      "\n",
      "Ep.00009 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00010 - TRN: loss 0.58 VAL: loss 0.61\n",
      "\n",
      "Ep.00010 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00011 - TRN: loss 0.57 VAL: loss 0.60\n",
      "\n",
      "Ep.00011 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00012 - TRN: loss 0.56 VAL: loss 0.60\n",
      "\n",
      "Ep.00012 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00013 - TRN: loss 0.56 VAL: loss 0.59\n",
      "\n",
      "Ep.00013 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00014 - TRN: loss 0.55 VAL: loss 0.59\n",
      "\n",
      "Ep.00014 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00015 - TRN: loss 0.55 VAL: loss 0.58\n",
      "\n",
      "Ep.00015 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00016 - TRN: loss 0.54 VAL: loss 0.58\n",
      "\n",
      "Ep.00016 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00017 - TRN: loss 0.54 VAL: loss 0.58\n",
      "\n",
      "Ep.00017 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00018 - TRN: loss 0.54 VAL: loss 0.57\n",
      "\n",
      "Ep.00018 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00019 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00019 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00020 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00020 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00021 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00021 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00022 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00022 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00023 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00023 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00024 - TRN: loss 0.53 VAL: loss 0.57\n",
      "\n",
      "Ep.00024 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00025 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00025 - TRN: acc 1.00 VAL: acc 0.91\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00026 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00026 - TRN: acc 1.00 VAL: acc 0.92\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00027 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00027 - TRN: acc 1.00 VAL: acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00028 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00028 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00029 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00029 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00030 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00030 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00031 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00031 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00032 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00032 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00033 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00033 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00034 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00034 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00035 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00035 - TRN: acc 1.00 VAL: acc 0.92\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00036 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00036 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00037 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00037 - TRN: acc 1.00 VAL: acc 0.91\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00038 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00038 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00039 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00039 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00040 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00040 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00041 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00041 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00042 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00042 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00043 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00043 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00044 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00044 - TRN: acc 1.00 VAL: acc 0.92\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00045 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00045 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00046 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00046 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00047 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00047 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00048 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00048 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00049 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00049 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00050 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00050 - TRN: acc 1.00 VAL: acc 0.91\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00051 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00051 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00052 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00052 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00053 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00053 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00054 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00054 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00055 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00055 - TRN: acc 1.00 VAL: acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00056 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00056 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00057 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00057 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00058 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00058 - TRN: acc 1.00 VAL: acc 0.91\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00059 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00059 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00060 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00060 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00061 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00061 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00062 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00062 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00063 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00063 - TRN: acc 1.00 VAL: acc 0.92\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00064 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00064 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00065 - TRN: loss 0.52 VAL: loss 0.58\n",
      "\n",
      "Ep.00065 - TRN: acc 1.00 VAL: acc 0.87\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00066 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00066 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00067 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00067 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00068 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00068 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00069 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00069 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00070 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00070 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00071 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00071 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00072 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00072 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00073 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00073 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00074 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00074 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00075 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00075 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00076 - TRN: loss 0.52 VAL: loss 0.57\n",
      "\n",
      "Ep.00076 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00077 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00077 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00078 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00078 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00079 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00079 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00080 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00080 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00081 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00081 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00082 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00082 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00083 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00083 - TRN: acc 1.00 VAL: acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00084 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00084 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00085 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00085 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00086 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00086 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00087 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00087 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00088 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00088 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00089 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00089 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00090 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00090 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00091 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00091 - TRN: acc 1.00 VAL: acc 0.92\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00092 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00092 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00093 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00093 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00094 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00094 - TRN: acc 1.00 VAL: acc 0.93\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00095 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00095 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00096 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00096 - TRN: acc 1.00 VAL: acc 0.94\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00097 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00097 - TRN: acc 1.00 VAL: acc 0.95\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00098 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00098 - TRN: acc 1.00 VAL: acc 0.96\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00099 - TRN: loss 0.52 VAL: loss 0.56\n",
      "\n",
      "Ep.00099 - TRN: acc 1.00 VAL: acc 0.93\n"
     ]
    }
   ],
   "source": [
    "#Actual train loop\n",
    "sess = tf.Session()\n",
    "\n",
    "# get the summary ready\n",
    "merged = tf.summary.merge_all()\n",
    "trn_writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "meta_info['tst_loss'] = []\n",
    "meta_info['tst_acc'] = []\n",
    "iteration = 0\n",
    "  \n",
    "for ep in range(num_epoch):\n",
    "    sess.run(trn_it.initializer)\n",
    "    sess.run(val_it.initializer)\n",
    "\n",
    "    trn_next_batch = trn_it.get_next()\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "    print('TRN:', flush=True)\n",
    "    while True: # loop to iterate over training set\n",
    "        try: \n",
    "            x, y = sess.run(trn_next_batch)\n",
    "        except: \n",
    "            break\n",
    "        print('+', end='', flush=True)\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        _, loss_i, acc_i, summary_i, _yhat, _pred = sess.run([train_op, loss_op, acc_op, merged, yhat_op, correct_pred], \n",
    "                                                   feed_dict={x_place:x, y_place:y, dp_place:dp_rate})\n",
    "\n",
    "        trn_writer.add_summary(summary_i, iteration)\n",
    "        avg_trn_loss = avg_trn_loss + loss_i\n",
    "        avg_trn_acc = avg_trn_acc + acc_i\n",
    "        \n",
    "        '''print(_yhat, flush=True)\n",
    "        print(y, flush=True)\n",
    "        print(_pred, flush=True)\n",
    "        print(acc_i, flush=True)'''\n",
    "    \n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "    avg_trn_acc = avg_trn_acc*batch_size/num_trn\n",
    "    \n",
    "    #Validation\n",
    "    avg_val_loss = 0\n",
    "    avg_val_acc = 0\n",
    "    print('VAL:', flush=True)\n",
    "    # evaluate the loss and accuracy\n",
    "    pred_and_gt = {}\n",
    "    pred_and_gt['x'] = []\n",
    "    pred_and_gt['y'] = []\n",
    "    pred_and_gt['yhat'] = []\n",
    "    \n",
    "    val_next_batch = val_it.get_next()\n",
    "\n",
    "    while True:\n",
    "        try: x, y = sess.run(val_next_batch)\n",
    "        except: break\n",
    "        print('+', end='', flush=True)\n",
    "        \n",
    "        # compute the loss\n",
    "        _yhat, _loss, _acc = sess.run([yhat_op, loss_op, acc_op], \n",
    "                                   feed_dict={x_place:x, y_place:y, dp_place:0.0})\n",
    "\n",
    "        # keep track of the loss/pred_and_gt\n",
    "        pred_and_gt['x'].append(x)\n",
    "        pred_and_gt['y'].append(y)\n",
    "        pred_and_gt['yhat'].append(_yhat)\n",
    "        avg_val_loss = avg_val_loss + _loss\n",
    "        avg_val_acc = avg_val_acc + _acc\n",
    "    \n",
    "    avg_val_loss = avg_val_loss*batch_size/num_val\n",
    "    avg_val_acc = avg_val_acc*batch_size/num_val\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    # concatenate the list to numpy array\n",
    "    for k, v in pred_and_gt.items():\n",
    "        pred_and_gt[k] = np.concatenate(v)\n",
    "    \n",
    "    \n",
    "    print('\\nEp.%05d - TRN: loss %.2f VAL: loss %.2f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "    print('\\nEp.%05d - TRN: acc %.2f VAL: acc %.2f' % (ep, avg_trn_acc, avg_val_acc), flush=True)\n",
    "        \n",
    "        \n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['trn_acc'].append(avg_trn_acc)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "    meta_info['val_acc'].append(avg_val_acc)\n",
    "    \n",
    "pkl.dump(meta_info, open(os.path.join('.', 'result.p'), 'wb'))\n",
    "saver.save(sess, \"./model/model\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.305\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x268d20f15c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_x\n",
      "target_y\n",
      "dropout_value\n",
      "fc/truncated_normal/shape\n",
      "fc/truncated_normal/mean\n",
      "fc/truncated_normal/stddev\n",
      "fc/truncated_normal/TruncatedNormal\n",
      "fc/truncated_normal/mul\n",
      "fc/truncated_normal\n",
      "fc/Variable\n",
      "fc/Variable/Assign\n",
      "fc/Variable/read\n",
      "fc/Const\n",
      "fc/Variable_1\n",
      "fc/Variable_1/Assign\n",
      "fc/Variable_1/read\n",
      "fc/MatMul\n",
      "fc/add\n",
      "LeakyRelu/alpha\n",
      "LeakyRelu/mul\n",
      "LeakyRelu/Maximum\n",
      "fc_1/truncated_normal/shape\n",
      "fc_1/truncated_normal/mean\n",
      "fc_1/truncated_normal/stddev\n",
      "fc_1/truncated_normal/TruncatedNormal\n",
      "fc_1/truncated_normal/mul\n",
      "fc_1/truncated_normal\n",
      "fc_1/Variable\n",
      "fc_1/Variable/Assign\n",
      "fc_1/Variable/read\n",
      "fc_1/Const\n",
      "fc_1/Variable_1\n",
      "fc_1/Variable_1/Assign\n",
      "fc_1/Variable_1/read\n",
      "fc_1/sub/x\n",
      "fc_1/sub\n",
      "fc_1/dropout/Shape\n",
      "fc_1/dropout/random_uniform/min\n",
      "fc_1/dropout/random_uniform/max\n",
      "fc_1/dropout/random_uniform/RandomUniform\n",
      "fc_1/dropout/random_uniform/sub\n",
      "fc_1/dropout/random_uniform/mul\n",
      "fc_1/dropout/random_uniform\n",
      "fc_1/dropout/add\n",
      "fc_1/dropout/Floor\n",
      "fc_1/dropout/div\n",
      "fc_1/dropout/mul\n",
      "fc_1/MatMul\n",
      "fc_1/add\n",
      "Sigmoid\n",
      "wd_term/L2Loss\n",
      "wd_term/L2Loss_1\n",
      "wd_term/L2Loss_2\n",
      "wd_term/L2Loss_3\n",
      "wd_term/stack\n",
      "wd_term/mul/x\n",
      "wd_term/mul\n",
      "crit_loss/logistic_loss/zeros_like\n",
      "crit_loss/logistic_loss/GreaterEqual\n",
      "crit_loss/logistic_loss/Select\n",
      "crit_loss/logistic_loss/Neg\n",
      "crit_loss/logistic_loss/Select_1\n",
      "crit_loss/logistic_loss/mul\n",
      "crit_loss/logistic_loss/sub\n",
      "crit_loss/logistic_loss/Exp\n",
      "crit_loss/logistic_loss/Log1p\n",
      "crit_loss/logistic_loss\n",
      "crit_loss/Const\n",
      "crit_loss/Mean\n",
      "crit_loss/Const_1\n",
      "crit_loss/Sum\n",
      "crit_loss/add\n",
      "Cross_Entropy_Loss/tags\n",
      "Cross_Entropy_Loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/crit_loss/add_grad/Shape\n",
      "gradients/crit_loss/add_grad/Shape_1\n",
      "gradients/crit_loss/add_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/add_grad/Sum\n",
      "gradients/crit_loss/add_grad/Reshape\n",
      "gradients/crit_loss/add_grad/Sum_1\n",
      "gradients/crit_loss/add_grad/Reshape_1\n",
      "gradients/crit_loss/add_grad/tuple/group_deps\n",
      "gradients/crit_loss/add_grad/tuple/control_dependency\n",
      "gradients/crit_loss/add_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/Mean_grad/Reshape/shape\n",
      "gradients/crit_loss/Mean_grad/Reshape\n",
      "gradients/crit_loss/Mean_grad/Shape\n",
      "gradients/crit_loss/Mean_grad/Tile\n",
      "gradients/crit_loss/Mean_grad/Shape_1\n",
      "gradients/crit_loss/Mean_grad/Shape_2\n",
      "gradients/crit_loss/Mean_grad/Const\n",
      "gradients/crit_loss/Mean_grad/Prod\n",
      "gradients/crit_loss/Mean_grad/Const_1\n",
      "gradients/crit_loss/Mean_grad/Prod_1\n",
      "gradients/crit_loss/Mean_grad/Maximum/y\n",
      "gradients/crit_loss/Mean_grad/Maximum\n",
      "gradients/crit_loss/Mean_grad/floordiv\n",
      "gradients/crit_loss/Mean_grad/Cast\n",
      "gradients/crit_loss/Mean_grad/truediv\n",
      "gradients/crit_loss/Sum_grad/Reshape/shape\n",
      "gradients/crit_loss/Sum_grad/Reshape\n",
      "gradients/crit_loss/Sum_grad/Tile/multiples\n",
      "gradients/crit_loss/Sum_grad/Tile\n",
      "gradients/crit_loss/logistic_loss_grad/Shape\n",
      "gradients/crit_loss/logistic_loss_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss_grad/Sum\n",
      "gradients/crit_loss/logistic_loss_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/control_dependency_1\n",
      "gradients/wd_term/mul_grad/Shape\n",
      "gradients/wd_term/mul_grad/Shape_1\n",
      "gradients/wd_term/mul_grad/BroadcastGradientArgs\n",
      "gradients/wd_term/mul_grad/mul\n",
      "gradients/wd_term/mul_grad/Sum\n",
      "gradients/wd_term/mul_grad/Reshape\n",
      "gradients/wd_term/mul_grad/mul_1\n",
      "gradients/wd_term/mul_grad/Sum_1\n",
      "gradients/wd_term/mul_grad/Reshape_1\n",
      "gradients/wd_term/mul_grad/tuple/group_deps\n",
      "gradients/wd_term/mul_grad/tuple/control_dependency\n",
      "gradients/wd_term/mul_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Shape\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Sum\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Neg\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/add/x\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/add\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/Reciprocal\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/mul\n",
      "gradients/wd_term/stack_grad/unstack\n",
      "gradients/wd_term/stack_grad/tuple/group_deps\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_1\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_2\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_3\n",
      "gradients/crit_loss/logistic_loss/Select_grad/zeros_like\n",
      "gradients/crit_loss/logistic_loss/Select_grad/Select\n",
      "gradients/crit_loss/logistic_loss/Select_grad/Select_1\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Shape\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss/mul_grad/mul\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Sum\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss/mul_grad/mul_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Exp_grad/mul\n",
      "gradients/wd_term/L2Loss_grad/mul\n",
      "gradients/wd_term/L2Loss_1_grad/mul\n",
      "gradients/wd_term/L2Loss_2_grad/mul\n",
      "gradients/wd_term/L2Loss_3_grad/mul\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/zeros_like\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/Select\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/Select_1\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Neg_grad/Neg\n",
      "gradients/AddN\n",
      "gradients/Sigmoid_grad/SigmoidGrad\n",
      "gradients/fc_1/add_grad/Shape\n",
      "gradients/fc_1/add_grad/Shape_1\n",
      "gradients/fc_1/add_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/add_grad/Sum\n",
      "gradients/fc_1/add_grad/Reshape\n",
      "gradients/fc_1/add_grad/Sum_1\n",
      "gradients/fc_1/add_grad/Reshape_1\n",
      "gradients/fc_1/add_grad/tuple/group_deps\n",
      "gradients/fc_1/add_grad/tuple/control_dependency\n",
      "gradients/fc_1/add_grad/tuple/control_dependency_1\n",
      "gradients/fc_1/MatMul_grad/MatMul\n",
      "gradients/fc_1/MatMul_grad/MatMul_1\n",
      "gradients/fc_1/MatMul_grad/tuple/group_deps\n",
      "gradients/fc_1/MatMul_grad/tuple/control_dependency\n",
      "gradients/fc_1/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_1\n",
      "gradients/fc_1/dropout/mul_grad/Shape\n",
      "gradients/fc_1/dropout/mul_grad/Shape_1\n",
      "gradients/fc_1/dropout/mul_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/dropout/mul_grad/mul\n",
      "gradients/fc_1/dropout/mul_grad/Sum\n",
      "gradients/fc_1/dropout/mul_grad/Reshape\n",
      "gradients/fc_1/dropout/mul_grad/mul_1\n",
      "gradients/fc_1/dropout/mul_grad/Sum_1\n",
      "gradients/fc_1/dropout/mul_grad/Reshape_1\n",
      "gradients/fc_1/dropout/mul_grad/tuple/group_deps\n",
      "gradients/fc_1/dropout/mul_grad/tuple/control_dependency\n",
      "gradients/fc_1/dropout/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_2\n",
      "gradients/fc_1/dropout/div_grad/Shape\n",
      "gradients/fc_1/dropout/div_grad/Shape_1\n",
      "gradients/fc_1/dropout/div_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/dropout/div_grad/RealDiv\n",
      "gradients/fc_1/dropout/div_grad/Sum\n",
      "gradients/fc_1/dropout/div_grad/Reshape\n",
      "gradients/fc_1/dropout/div_grad/Neg\n",
      "gradients/fc_1/dropout/div_grad/RealDiv_1\n",
      "gradients/fc_1/dropout/div_grad/RealDiv_2\n",
      "gradients/fc_1/dropout/div_grad/mul\n",
      "gradients/fc_1/dropout/div_grad/Sum_1\n",
      "gradients/fc_1/dropout/div_grad/Reshape_1\n",
      "gradients/fc_1/dropout/div_grad/tuple/group_deps\n",
      "gradients/fc_1/dropout/div_grad/tuple/control_dependency\n",
      "gradients/fc_1/dropout/div_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu/Maximum_grad/Shape\n",
      "gradients/LeakyRelu/Maximum_grad/Shape_1\n",
      "gradients/LeakyRelu/Maximum_grad/Shape_2\n",
      "gradients/LeakyRelu/Maximum_grad/zeros/Const\n",
      "gradients/LeakyRelu/Maximum_grad/zeros\n",
      "gradients/LeakyRelu/Maximum_grad/GreaterEqual\n",
      "gradients/LeakyRelu/Maximum_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu/Maximum_grad/Select\n",
      "gradients/LeakyRelu/Maximum_grad/Select_1\n",
      "gradients/LeakyRelu/Maximum_grad/Sum\n",
      "gradients/LeakyRelu/Maximum_grad/Reshape\n",
      "gradients/LeakyRelu/Maximum_grad/Sum_1\n",
      "gradients/LeakyRelu/Maximum_grad/Reshape_1\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/group_deps\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu/mul_grad/Shape\n",
      "gradients/LeakyRelu/mul_grad/Shape_1\n",
      "gradients/LeakyRelu/mul_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu/mul_grad/mul\n",
      "gradients/LeakyRelu/mul_grad/Sum\n",
      "gradients/LeakyRelu/mul_grad/Reshape\n",
      "gradients/LeakyRelu/mul_grad/mul_1\n",
      "gradients/LeakyRelu/mul_grad/Sum_1\n",
      "gradients/LeakyRelu/mul_grad/Reshape_1\n",
      "gradients/LeakyRelu/mul_grad/tuple/group_deps\n",
      "gradients/LeakyRelu/mul_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_3\n",
      "gradients/fc/add_grad/Shape\n",
      "gradients/fc/add_grad/Shape_1\n",
      "gradients/fc/add_grad/BroadcastGradientArgs\n",
      "gradients/fc/add_grad/Sum\n",
      "gradients/fc/add_grad/Reshape\n",
      "gradients/fc/add_grad/Sum_1\n",
      "gradients/fc/add_grad/Reshape_1\n",
      "gradients/fc/add_grad/tuple/group_deps\n",
      "gradients/fc/add_grad/tuple/control_dependency\n",
      "gradients/fc/add_grad/tuple/control_dependency_1\n",
      "gradients/fc/MatMul_grad/MatMul\n",
      "gradients/fc/MatMul_grad/MatMul_1\n",
      "gradients/fc/MatMul_grad/tuple/group_deps\n",
      "gradients/fc/MatMul_grad/tuple/control_dependency\n",
      "gradients/fc/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_4\n",
      "gradients/AddN_5\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "fc/Variable/Adam/Initializer/zeros\n",
      "fc/Variable/Adam\n",
      "fc/Variable/Adam/Assign\n",
      "fc/Variable/Adam/read\n",
      "fc/Variable/Adam_1/Initializer/zeros\n",
      "fc/Variable/Adam_1\n",
      "fc/Variable/Adam_1/Assign\n",
      "fc/Variable/Adam_1/read\n",
      "fc/Variable_1/Adam/Initializer/zeros\n",
      "fc/Variable_1/Adam\n",
      "fc/Variable_1/Adam/Assign\n",
      "fc/Variable_1/Adam/read\n",
      "fc/Variable_1/Adam_1/Initializer/zeros\n",
      "fc/Variable_1/Adam_1\n",
      "fc/Variable_1/Adam_1/Assign\n",
      "fc/Variable_1/Adam_1/read\n",
      "fc_1/Variable/Adam/Initializer/zeros\n",
      "fc_1/Variable/Adam\n",
      "fc_1/Variable/Adam/Assign\n",
      "fc_1/Variable/Adam/read\n",
      "fc_1/Variable/Adam_1/Initializer/zeros\n",
      "fc_1/Variable/Adam_1\n",
      "fc_1/Variable/Adam_1/Assign\n",
      "fc_1/Variable/Adam_1/read\n",
      "fc_1/Variable_1/Adam/Initializer/zeros\n",
      "fc_1/Variable_1/Adam\n",
      "fc_1/Variable_1/Adam/Assign\n",
      "fc_1/Variable_1/Adam/read\n",
      "fc_1/Variable_1/Adam_1/Initializer/zeros\n",
      "fc_1/Variable_1/Adam_1\n",
      "fc_1/Variable_1/Adam_1/Assign\n",
      "fc_1/Variable_1/Adam_1/read\n",
      "Adam/learning_rate\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_fc/Variable/ApplyAdam\n",
      "Adam/update_fc/Variable_1/ApplyAdam\n",
      "Adam/update_fc_1/Variable/ApplyAdam\n",
      "Adam/update_fc_1/Variable_1/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "Round\n",
      "Equal\n",
      "Cast\n",
      "Const\n",
      "Mean\n",
      "Accuracy/tags\n",
      "Accuracy\n",
      "dataset/tensors/component_0\n",
      "dataset/tensors/component_1\n",
      "dataset/buffer_size\n",
      "dataset/seed\n",
      "dataset/seed2\n",
      "Iterator\n",
      "TensorSliceDataset\n",
      "ShuffleDataset\n",
      "BatchDataset/batch_size\n",
      "BatchDataset\n",
      "MakeIterator\n",
      "dataset_1/tensors/component_0\n",
      "dataset_1/tensors/component_1\n",
      "Iterator_1\n",
      "TensorSliceDataset_1\n",
      "BatchDataset_1/batch_size\n",
      "BatchDataset_1\n",
      "MakeIterator_1\n",
      "Merge/MergeSummary\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/RestoreV2_12/tensor_names\n",
      "save/RestoreV2_12/shape_and_slices\n",
      "save/RestoreV2_12\n",
      "save/Assign_12\n",
      "save/RestoreV2_13/tensor_names\n",
      "save/RestoreV2_13/shape_and_slices\n",
      "save/RestoreV2_13\n",
      "save/Assign_13\n",
      "save/restore_all\n",
      "init\n",
      "IteratorGetNext\n",
      "IteratorGetNext_1\n",
      "IteratorGetNext_2\n",
      "IteratorGetNext_3\n",
      "IteratorGetNext_4\n",
      "IteratorGetNext_5\n",
      "IteratorGetNext_6\n",
      "IteratorGetNext_7\n",
      "IteratorGetNext_8\n",
      "IteratorGetNext_9\n",
      "IteratorGetNext_10\n",
      "IteratorGetNext_11\n",
      "IteratorGetNext_12\n",
      "IteratorGetNext_13\n",
      "IteratorGetNext_14\n",
      "IteratorGetNext_15\n",
      "IteratorGetNext_16\n",
      "IteratorGetNext_17\n",
      "IteratorGetNext_18\n",
      "IteratorGetNext_19\n",
      "IteratorGetNext_20\n",
      "IteratorGetNext_21\n",
      "IteratorGetNext_22\n",
      "IteratorGetNext_23\n",
      "IteratorGetNext_24\n",
      "IteratorGetNext_25\n",
      "IteratorGetNext_26\n",
      "IteratorGetNext_27\n",
      "IteratorGetNext_28\n",
      "IteratorGetNext_29\n",
      "IteratorGetNext_30\n",
      "IteratorGetNext_31\n",
      "IteratorGetNext_32\n",
      "IteratorGetNext_33\n",
      "IteratorGetNext_34\n",
      "IteratorGetNext_35\n",
      "IteratorGetNext_36\n",
      "IteratorGetNext_37\n",
      "IteratorGetNext_38\n",
      "IteratorGetNext_39\n",
      "IteratorGetNext_40\n",
      "IteratorGetNext_41\n",
      "IteratorGetNext_42\n",
      "IteratorGetNext_43\n",
      "IteratorGetNext_44\n",
      "IteratorGetNext_45\n",
      "IteratorGetNext_46\n",
      "IteratorGetNext_47\n",
      "IteratorGetNext_48\n",
      "IteratorGetNext_49\n",
      "IteratorGetNext_50\n",
      "IteratorGetNext_51\n",
      "IteratorGetNext_52\n",
      "IteratorGetNext_53\n",
      "IteratorGetNext_54\n",
      "IteratorGetNext_55\n",
      "IteratorGetNext_56\n",
      "IteratorGetNext_57\n",
      "IteratorGetNext_58\n",
      "IteratorGetNext_59\n",
      "IteratorGetNext_60\n",
      "IteratorGetNext_61\n",
      "IteratorGetNext_62\n",
      "IteratorGetNext_63\n",
      "IteratorGetNext_64\n",
      "IteratorGetNext_65\n",
      "IteratorGetNext_66\n",
      "IteratorGetNext_67\n",
      "IteratorGetNext_68\n",
      "IteratorGetNext_69\n",
      "IteratorGetNext_70\n",
      "IteratorGetNext_71\n",
      "IteratorGetNext_72\n",
      "IteratorGetNext_73\n",
      "IteratorGetNext_74\n",
      "IteratorGetNext_75\n",
      "IteratorGetNext_76\n",
      "IteratorGetNext_77\n",
      "IteratorGetNext_78\n",
      "IteratorGetNext_79\n",
      "IteratorGetNext_80\n",
      "IteratorGetNext_81\n",
      "IteratorGetNext_82\n",
      "IteratorGetNext_83\n",
      "IteratorGetNext_84\n",
      "IteratorGetNext_85\n",
      "IteratorGetNext_86\n",
      "IteratorGetNext_87\n",
      "IteratorGetNext_88\n",
      "IteratorGetNext_89\n",
      "IteratorGetNext_90\n",
      "IteratorGetNext_91\n",
      "IteratorGetNext_92\n",
      "IteratorGetNext_93\n",
      "IteratorGetNext_94\n",
      "IteratorGetNext_95\n",
      "IteratorGetNext_96\n",
      "IteratorGetNext_97\n",
      "IteratorGetNext_98\n",
      "IteratorGetNext_99\n",
      "IteratorGetNext_100\n",
      "IteratorGetNext_101\n",
      "IteratorGetNext_102\n",
      "IteratorGetNext_103\n",
      "IteratorGetNext_104\n",
      "IteratorGetNext_105\n",
      "IteratorGetNext_106\n",
      "IteratorGetNext_107\n",
      "IteratorGetNext_108\n",
      "IteratorGetNext_109\n",
      "IteratorGetNext_110\n",
      "IteratorGetNext_111\n",
      "IteratorGetNext_112\n",
      "IteratorGetNext_113\n",
      "IteratorGetNext_114\n",
      "IteratorGetNext_115\n",
      "IteratorGetNext_116\n",
      "IteratorGetNext_117\n",
      "IteratorGetNext_118\n",
      "IteratorGetNext_119\n",
      "IteratorGetNext_120\n",
      "IteratorGetNext_121\n",
      "IteratorGetNext_122\n",
      "IteratorGetNext_123\n",
      "IteratorGetNext_124\n",
      "IteratorGetNext_125\n",
      "IteratorGetNext_126\n",
      "IteratorGetNext_127\n",
      "IteratorGetNext_128\n",
      "IteratorGetNext_129\n",
      "IteratorGetNext_130\n",
      "IteratorGetNext_131\n",
      "IteratorGetNext_132\n",
      "IteratorGetNext_133\n",
      "IteratorGetNext_134\n",
      "IteratorGetNext_135\n",
      "IteratorGetNext_136\n",
      "IteratorGetNext_137\n",
      "IteratorGetNext_138\n",
      "IteratorGetNext_139\n",
      "IteratorGetNext_140\n",
      "IteratorGetNext_141\n",
      "IteratorGetNext_142\n",
      "IteratorGetNext_143\n",
      "IteratorGetNext_144\n",
      "IteratorGetNext_145\n",
      "IteratorGetNext_146\n",
      "IteratorGetNext_147\n",
      "IteratorGetNext_148\n",
      "IteratorGetNext_149\n",
      "IteratorGetNext_150\n",
      "IteratorGetNext_151\n",
      "IteratorGetNext_152\n",
      "IteratorGetNext_153\n",
      "IteratorGetNext_154\n",
      "IteratorGetNext_155\n",
      "IteratorGetNext_156\n",
      "IteratorGetNext_157\n",
      "IteratorGetNext_158\n",
      "IteratorGetNext_159\n",
      "IteratorGetNext_160\n",
      "IteratorGetNext_161\n",
      "IteratorGetNext_162\n",
      "IteratorGetNext_163\n",
      "IteratorGetNext_164\n",
      "IteratorGetNext_165\n",
      "IteratorGetNext_166\n",
      "IteratorGetNext_167\n",
      "IteratorGetNext_168\n",
      "IteratorGetNext_169\n",
      "IteratorGetNext_170\n",
      "IteratorGetNext_171\n",
      "IteratorGetNext_172\n",
      "IteratorGetNext_173\n",
      "IteratorGetNext_174\n",
      "IteratorGetNext_175\n",
      "IteratorGetNext_176\n",
      "IteratorGetNext_177\n",
      "IteratorGetNext_178\n",
      "IteratorGetNext_179\n",
      "IteratorGetNext_180\n",
      "IteratorGetNext_181\n",
      "IteratorGetNext_182\n",
      "IteratorGetNext_183\n",
      "IteratorGetNext_184\n",
      "IteratorGetNext_185\n",
      "IteratorGetNext_186\n",
      "IteratorGetNext_187\n",
      "IteratorGetNext_188\n",
      "IteratorGetNext_189\n",
      "IteratorGetNext_190\n",
      "IteratorGetNext_191\n",
      "IteratorGetNext_192\n",
      "IteratorGetNext_193\n",
      "IteratorGetNext_194\n",
      "IteratorGetNext_195\n",
      "IteratorGetNext_196\n",
      "IteratorGetNext_197\n",
      "IteratorGetNext_198\n",
      "IteratorGetNext_199\n",
      "INFO:tensorflow:Restoring parameters from ./model/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00020623]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Little test\n",
    "print(pws_1, pws_2)\n",
    "tst_x_matrix = w_s_network(num_node, pws_1, avg_degree)\n",
    "plt.imshow(tst_x_matrix, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "tst_x = tst_x_matrix.reshape(1, -1)\n",
    "truncated_tst_x = np.zeros((1, input_size))\n",
    "start = 0\n",
    "for j in range(num_node):\n",
    "    start = start + j\n",
    "    truncated_tst_x[0, start:start+j] = tst_x[0, j*num_node : j*num_node + j]\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "imported_graph = tf.train.import_meta_graph('./model/model.meta')\n",
    "\n",
    "for tensor in tf.get_default_graph().get_operations():\n",
    "    print (tensor.name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    imported_graph.restore(sess, './model/model')\n",
    "    yhat = sess.run(['Sigmoid:0'], feed_dict={'input_x:0':truncated_tst_x, 'dropout_value:0':0.0})\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
