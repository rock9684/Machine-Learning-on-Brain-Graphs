{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn as nl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the rewiring probabilities of two groups\n",
    "pws_1 = 0.1\n",
    "pws_2 = 0.15\n",
    "num_trn = 800\n",
    "num_val = 200\n",
    "num_node = 50\n",
    "avg_degree = 10\n",
    "\n",
    "#Computation graph params\n",
    "num_layer = 2\n",
    "input_size = 64\n",
    "output_size = 1\n",
    "num_hidden_units = 30\n",
    "dp_rate = 0\n",
    "wd_rate = 0.005\n",
    "l_r = 0.01\n",
    "num_epoch = 200\n",
    "batch_size = 10\n",
    "\n",
    "np.random.seed((int)(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate Watts-Strogatz Networks\n",
    "def w_s_network(num_node, pws, avg_degree):\n",
    "    adjacency_matrix = np.zeros((num_node + 1, num_node + 1))\n",
    "    \n",
    "    #Connect each node to avg_degree/2 nearest neighbour (eg. node 0 will connect to node 1 and node num_node-1)\n",
    "    half_k = (int)(avg_degree/2)\n",
    "    \n",
    "    for node_i in range(num_node):\n",
    "        start = node_i - half_k\n",
    "        index = start - 1\n",
    "        for j in range(avg_degree):\n",
    "            index = index + 1\n",
    "            if index == node_i:\n",
    "                index = index + 1\n",
    "            if index >= num_node:\n",
    "                index = index - num_node\n",
    "            if adjacency_matrix[node_i, index] != 1:\n",
    "                adjacency_matrix[node_i, index] = 1\n",
    "    \n",
    "    #Rewiring connections\n",
    "    prob_matrix = np.random.rand(num_node, num_node)\n",
    "    for node_i in range(num_node):\n",
    "        prob_matrix[node_i, node_i] = 1\n",
    "    \n",
    "    for node_i in range(num_node):\n",
    "        for node_j in range(num_node):\n",
    "            if adjacency_matrix[node_i, node_j] == 1 and prob_matrix[node_i, node_j] < pws:\n",
    "                adjacency_matrix[node_i, node_j] = 0\n",
    "                adjacency_matrix[node_j, node_i] = 0\n",
    "                \n",
    "                rewired = False\n",
    "                while rewired == False:\n",
    "                    index_i = np.random.randint(num_node)\n",
    "                    index_j = np.random.randint(num_node)\n",
    "                    if index_i != index_j and adjacency_matrix[index_i, index_j] == 0:\n",
    "                        adjacency_matrix[index_i, index_j] = 1\n",
    "                        adjacency_matrix[index_j, index_i] = 1\n",
    "                        rewired = True\n",
    "    \n",
    "    #Special node connects to every node\n",
    "    adjacency_matrix[num_node] = np.ones(num_node+1)\n",
    "    adjacency_matrix[num_node, -1] = 0\n",
    "    \n",
    "    for i in range(num_node):\n",
    "        adjacency_matrix[i, -1] = 1\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate 1000 samples (800 trn and 200 val)\n",
    "#Each sample has 50 nodes and each node has average degree of 8\n",
    "data = {}\n",
    "\n",
    "trn_x = np.zeros((num_trn, (num_node+1) * (num_node+1)))\n",
    "trn_y = np.zeros((num_trn, 1))\n",
    "val_x = np.zeros((num_val, (num_node+1) * (num_node+1)))\n",
    "val_y = np.zeros((num_val, 1))\n",
    "\n",
    "for i in range(num_trn):\n",
    "    group = np.random.randint(2)\n",
    "    if group == 0:\n",
    "        sample_x = w_s_network(50, pws_1, avg_degree)\n",
    "        sample_y = 0\n",
    "    \n",
    "    if group == 1:\n",
    "        sample_x = w_s_network(50, pws_2, avg_degree)\n",
    "        sample_y = 1\n",
    "    \n",
    "    trn_x[i, :] = sample_x.reshape(1, -1)\n",
    "    trn_y[i] = sample_y\n",
    "\n",
    "for i in range(num_val):\n",
    "    group = np.random.randint(2)\n",
    "    if group == 0:\n",
    "        sample_x = w_s_network(50, pws_1, avg_degree)\n",
    "        sample_y = 0\n",
    "    \n",
    "    if group == 1:\n",
    "        sample_x = w_s_network(50, pws_2, avg_degree)\n",
    "        sample_y = 1\n",
    "    \n",
    "    val_x[i, :] = sample_x.reshape(1, -1)\n",
    "    val_y[i] = sample_y\n",
    "\n",
    "data['trn_x'] = trn_x\n",
    "data['trn_y'] = trn_y\n",
    "data['val_x'] = val_x\n",
    "data['val_y'] = val_y\n",
    "\n",
    "pkl.dump(data, open(os.path.join('.', 'data.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Truncate each sample to save only the bottom left part\\ntruncated_vector_size = (int)((1+num_node)*num_node/2)\\ntruncated_data = {}\\ntruncated_data['trn_y'] = data['trn_y']\\ntruncated_data['val_y'] = data['val_y']\\ntruncated_data['trn_x'] = np.zeros((num_trn, truncated_vector_size))\\ntruncated_data['val_x'] = np.zeros((num_val, truncated_vector_size))\\nfor i in range(num_trn):\\n    truncated_trn_x = np.zeros((1, truncated_vector_size))\\n    start = 0\\n    for j in range(num_node):\\n        start = start + j\\n        truncated_trn_x[0, start:start+j] = data['trn_x'][i, j*num_node : j*num_node + j]\\n    truncated_data['trn_x'][i, ] = truncated_trn_x\\n\\nfor i in range(num_val):\\n    truncated_val_x = np.zeros((1, truncated_vector_size))\\n    start = 0\\n    for j in range(num_node):\\n        start = start + j\\n        truncated_val_x[0, start:start+j] = data['val_x'][i, j*num_node : j*num_node + j]\\n    truncated_data['val_x'][i, ] = truncated_val_x\\n    \\npkl.dump(truncated_data, open(os.path.join('.', 'truncated_data.p'), 'wb'))\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Truncate each sample to save only the bottom left part\n",
    "truncated_vector_size = (int)((1+num_node)*num_node/2)\n",
    "truncated_data = {}\n",
    "truncated_data['trn_y'] = data['trn_y']\n",
    "truncated_data['val_y'] = data['val_y']\n",
    "truncated_data['trn_x'] = np.zeros((num_trn, truncated_vector_size))\n",
    "truncated_data['val_x'] = np.zeros((num_val, truncated_vector_size))\n",
    "for i in range(num_trn):\n",
    "    truncated_trn_x = np.zeros((1, truncated_vector_size))\n",
    "    start = 0\n",
    "    for j in range(num_node):\n",
    "        start = start + j\n",
    "        truncated_trn_x[0, start:start+j] = data['trn_x'][i, j*num_node : j*num_node + j]\n",
    "    truncated_data['trn_x'][i, ] = truncated_trn_x\n",
    "\n",
    "for i in range(num_val):\n",
    "    truncated_val_x = np.zeros((1, truncated_vector_size))\n",
    "    start = 0\n",
    "    for j in range(num_node):\n",
    "        start = start + j\n",
    "        truncated_val_x[0, start:start+j] = data['val_x'][i, j*num_node : j*num_node + j]\n",
    "    truncated_data['val_x'][i, ] = truncated_val_x\n",
    "    \n",
    "pkl.dump(truncated_data, open(os.path.join('.', 'truncated_data.p'), 'wb'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x292e464ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29114e3a5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_t = pkl.load(open( \"data.p\", \"rb\" ))\n",
    "\n",
    "trn_sample = data_t['trn_x'][55, ]\n",
    "print(data_t['trn_y'][55])\n",
    "val_sample = data_t['val_x'][117, ]\n",
    "print(data_t['val_y'][117])\n",
    "\n",
    "#m = m.reshape(50, -1)\n",
    "\n",
    "plt.imshow(trn_sample.reshape(51, 51), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(val_sample.reshape(51, 51), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(cur_layer, out_size, dp_rate, is_input):\n",
    "    \n",
    "    _, cur_size = [item.value for item in cur_layer.shape]\n",
    "        \n",
    "    var = 2.0 / (cur_size + out_size)\n",
    "\n",
    "    stddev = math.sqrt(var)\n",
    "\n",
    "    print(\"steddev is\", stddev)\n",
    "    print(out_size)\n",
    "\n",
    "    with tf.name_scope('fc'):\n",
    "        W_fc = tf.Variable(tf.truncated_normal(shape=[cur_size,out_size], stddev=stddev))\n",
    "        b_fc = tf.Variable(tf.constant(0.0, shape=[out_size]))\n",
    "        if is_input is False: \n",
    "            cur_layer = tf.nn.dropout(cur_layer, 1 - dp_rate)\n",
    "        out_layer = tf.matmul(cur_layer, W_fc) + b_fc\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "def activation(cur_layer, is_output=False):\n",
    "    if is_output is True:\n",
    "        return tf.nn.sigmoid(cur_layer)\n",
    "    return tf.nn.leaky_relu(cur_layer)\n",
    "\n",
    "def loss_function(yhat, y):\n",
    "    '''\n",
    "    '''\n",
    "    # get the weight decay terms\n",
    "    with tf.name_scope('wd_term'):\n",
    "        wght_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='fc')\n",
    "        wd_term = wd_rate * tf.stack([tf.nn.l2_loss(i) for i in wght_params])\n",
    "\n",
    "    with tf.name_scope('crit_loss'):\n",
    "            crit = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=yhat)) + tf.reduce_sum(wd_term)\n",
    "\n",
    "    return crit\n",
    "\n",
    "def generate_dataset(x, y, shuffle=True):\n",
    "    with tf.name_scope('dataset'):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        if shuffle: \n",
    "            dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(?, 64)\n",
      "steddev is 0.14586499149789456\n",
      "30\n",
      "1\n",
      "(?, 30)\n",
      "steddev is 0.254000254000381\n",
      "1\n",
      "[-0.00093709  0.06942902 -0.05619589  0.16517994 -0.17297737 -0.00676474\n",
      " -0.02693254  0.17650674  0.2622626  -0.11422807 -0.17502528  0.06282581\n",
      "  0.10598552  0.06969111  0.17957872  0.18309157 -0.04332064 -0.19446452\n",
      "  0.01848491  0.14546336  0.11131436  0.383486   -0.05562869  0.10139539\n",
      " -0.08666497  0.06698841  0.10583346 -0.05387706  0.17989753  0.02894785\n",
      " -0.00431256  0.11335865  0.07675672  0.11537535 -0.16748744 -0.04790552\n",
      " -0.10640086 -0.02810473  0.20054193  0.21094486 -0.02591559 -0.05237211\n",
      "  0.1736602   0.1381509   0.14549562 -0.07799649  0.01332354 -0.24899514\n",
      "  0.07423684  0.01397856  0.09848736  0.00600032  0.02055552 -0.11788579\n",
      "  0.0279994  -0.14649764  0.16131745 -0.1170468  -0.11076677  0.18425508\n",
      " -0.00067086 -0.07033493  0.14320983 -0.09070077]\n"
     ]
    }
   ],
   "source": [
    "#Build the actual graph\n",
    "tf.reset_default_graph()\n",
    "x_place = tf.placeholder('float', [None, input_size], name='input_x')\n",
    "y_place = tf.placeholder('float', [None,  output_size], name='target_y')\n",
    "dp_place = tf.placeholder(tf.float32, name='dropout_value')\n",
    "\n",
    "cur_layer = x_place\n",
    "\n",
    "for i in range(num_layer):\n",
    "    print(i)\n",
    "    if i != (num_layer - 1):\n",
    "        print(cur_layer.shape, flush=True)\n",
    "        cur_layer = forward_prop(cur_layer, num_hidden_units, dp_place, i == 0)\n",
    "        cur_layer = activation(cur_layer)\n",
    "    else:\n",
    "        print(cur_layer.shape, flush=True)\n",
    "        cur_layer = forward_prop(cur_layer, output_size, dp_place, i == 0)\n",
    "        yhat_op = activation(cur_layer, True)\n",
    "\n",
    "#Cross entropy loss\n",
    "loss_op = loss_function(yhat_op, y_place)\n",
    "tf.summary.scalar('Cross_Entropy_Loss', loss_op)\n",
    "\n",
    "#adam\n",
    "optimizer = tf.train.AdamOptimizer(l_r)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred = tf.equal(y_place, tf.round(yhat_op)) \n",
    "acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('Accuracy', acc_op)\n",
    "\n",
    "#Iterators\n",
    "deepwalk_data = pkl.load(open( \"deepwalk_data.p\", \"rb\" ))\n",
    "print(deepwalk_data['trn_x'][1])\n",
    "trn_it = generate_dataset(deepwalk_data['trn_x'], deepwalk_data['trn_y'], shuffle=True)\n",
    "val_it = generate_dataset(deepwalk_data['val_x'], deepwalk_data['val_y'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00000 - TRN: loss 0.71 VAL: loss 0.72\n",
      "\n",
      "Ep.00000 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00001 - TRN: loss 0.70 VAL: loss 0.72\n",
      "\n",
      "Ep.00001 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00002 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00002 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00003 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00003 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00004 - TRN: loss 0.70 VAL: loss 0.72\n",
      "\n",
      "Ep.00004 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00005 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00005 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00006 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00006 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00007 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00007 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00008 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00008 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00009 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00009 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00010 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00010 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00011 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00011 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00012 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00012 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00013 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00013 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00014 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00014 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00015 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00015 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00016 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00016 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00017 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00017 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00018 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00018 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00019 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00019 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00020 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00020 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00021 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00021 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00022 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00022 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00023 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00023 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00024 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00024 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00025 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00025 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00026 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00026 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00027 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00027 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00028 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00028 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00029 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00029 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00030 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00030 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00031 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00031 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00032 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00032 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00033 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00033 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00034 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00034 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00035 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00035 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00036 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00036 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00037 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00037 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00038 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00038 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00039 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00039 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00040 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00040 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00041 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00041 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00042 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00042 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00043 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00043 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00044 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00044 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00045 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00045 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00046 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00046 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00047 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00047 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00048 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00048 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00049 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00049 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00050 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00050 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00051 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00051 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00052 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00052 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00053 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00053 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00054 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00054 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00055 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00055 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00056 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00056 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00057 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00057 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00058 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00058 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00059 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00059 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00060 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00060 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00061 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00061 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00062 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00062 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00063 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00063 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00064 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00064 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00065 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00065 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00066 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00066 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00067 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00067 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00068 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00068 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00069 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00069 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00070 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00070 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00071 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00071 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00072 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00072 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00073 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00073 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00074 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00074 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00075 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00075 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00076 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00076 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00077 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00077 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00078 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00078 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00079 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00079 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00080 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00080 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00081 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00081 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00082 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00082 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00083 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00083 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00084 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00084 - TRN: acc 0.47 VAL: acc 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00085 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00085 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00086 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00086 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00087 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00087 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00088 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00088 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00089 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00089 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00090 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00090 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00091 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00091 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00092 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00092 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00093 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00093 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00094 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00094 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00095 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00095 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00096 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00096 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00097 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00097 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00098 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00098 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00099 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00099 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00100 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00100 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00101 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00101 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00102 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00102 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00103 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00103 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00104 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00104 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00105 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00105 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00106 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00106 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00107 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00107 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00108 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00108 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00109 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00109 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00110 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00110 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00111 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00111 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00112 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00112 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00113 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00113 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00114 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00114 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00115 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00115 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00116 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00116 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00117 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00117 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00118 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00118 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00119 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00119 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00120 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00120 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00121 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00121 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00122 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00122 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00123 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00123 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00124 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00124 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00125 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00125 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00126 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00126 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00127 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00127 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00128 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00128 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00129 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00129 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00130 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00130 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00131 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00131 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00132 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00132 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00133 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00133 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00134 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00134 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00135 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00135 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00136 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00136 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00137 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00137 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00138 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00138 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00139 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00139 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00140 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00140 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00141 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00141 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00142 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00142 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00143 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00143 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00144 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00144 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00145 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00145 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00146 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00146 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00147 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00147 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00148 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00148 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00149 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00149 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00150 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00150 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00151 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00151 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00152 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00152 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00153 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00153 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00154 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00154 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00155 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00155 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00156 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00156 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00157 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00157 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00158 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00158 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00159 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00159 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00160 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00160 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00161 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00161 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00162 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00162 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00163 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00163 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00164 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00164 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00165 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00165 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00166 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00166 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00167 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00167 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00168 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00168 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00169 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00169 - TRN: acc 0.47 VAL: acc 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00170 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00170 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00171 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00171 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00172 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00172 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00173 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00173 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00174 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00174 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00175 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00175 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00176 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00176 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00177 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00177 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00178 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00178 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00179 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00179 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00180 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00180 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00181 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00181 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00182 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00182 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00183 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00183 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00184 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00184 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00185 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00185 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00186 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00186 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00187 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00187 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00188 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00188 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00189 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00189 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00190 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00190 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00191 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00191 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00192 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00192 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00193 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00193 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00194 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00194 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00195 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00195 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00196 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00196 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00197 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00197 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00198 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00198 - TRN: acc 0.47 VAL: acc 0.54\n",
      "TRN:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++VAL:\n",
      "++++++++++++++++++++\n",
      "\n",
      "Ep.00199 - TRN: loss 0.70 VAL: loss 0.71\n",
      "\n",
      "Ep.00199 - TRN: acc 0.47 VAL: acc 0.54\n"
     ]
    }
   ],
   "source": [
    "#Actual train loop\n",
    "sess = tf.Session()\n",
    "\n",
    "# get the summary ready\n",
    "merged = tf.summary.merge_all()\n",
    "trn_writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "meta_info['tst_loss'] = []\n",
    "meta_info['tst_acc'] = []\n",
    "iteration = 0\n",
    "  \n",
    "for ep in range(num_epoch):\n",
    "    sess.run(trn_it.initializer)\n",
    "    sess.run(val_it.initializer)\n",
    "\n",
    "    trn_next_batch = trn_it.get_next()\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "    print('TRN:', flush=True)\n",
    "    while True: # loop to iterate over training set\n",
    "        try: \n",
    "            x, y = sess.run(trn_next_batch)\n",
    "        except: \n",
    "            break\n",
    "        print('+', end='', flush=True)\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        _, loss_i, acc_i, summary_i, _yhat, _pred = sess.run([train_op, loss_op, acc_op, merged, yhat_op, correct_pred], \n",
    "                                                   feed_dict={x_place:x, y_place:y, dp_place:dp_rate})\n",
    "\n",
    "        trn_writer.add_summary(summary_i, iteration)\n",
    "        avg_trn_loss = avg_trn_loss + loss_i\n",
    "        avg_trn_acc = avg_trn_acc + acc_i\n",
    "        \n",
    "        '''print(_yhat, flush=True)\n",
    "        print(y, flush=True)\n",
    "        print(_pred, flush=True)\n",
    "        print(acc_i, flush=True)'''\n",
    "    \n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "    avg_trn_acc = avg_trn_acc*batch_size/num_trn\n",
    "    \n",
    "    #Validation\n",
    "    avg_val_loss = 0\n",
    "    avg_val_acc = 0\n",
    "    print('VAL:', flush=True)\n",
    "    # evaluate the loss and accuracy\n",
    "    pred_and_gt = {}\n",
    "    pred_and_gt['x'] = []\n",
    "    pred_and_gt['y'] = []\n",
    "    pred_and_gt['yhat'] = []\n",
    "    \n",
    "    val_next_batch = val_it.get_next()\n",
    "\n",
    "    while True:\n",
    "        try: x, y = sess.run(val_next_batch)\n",
    "        except: break\n",
    "        print('+', end='', flush=True)\n",
    "        \n",
    "        # compute the loss\n",
    "        _yhat, _loss, _acc = sess.run([yhat_op, loss_op, acc_op], \n",
    "                                   feed_dict={x_place:x, y_place:y, dp_place:0.0})\n",
    "\n",
    "        # keep track of the loss/pred_and_gt\n",
    "        pred_and_gt['x'].append(x)\n",
    "        pred_and_gt['y'].append(y)\n",
    "        pred_and_gt['yhat'].append(_yhat)\n",
    "        avg_val_loss = avg_val_loss + _loss\n",
    "        avg_val_acc = avg_val_acc + _acc\n",
    "    \n",
    "    avg_val_loss = avg_val_loss*batch_size/num_val\n",
    "    avg_val_acc = avg_val_acc*batch_size/num_val\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    # concatenate the list to numpy array\n",
    "    for k, v in pred_and_gt.items():\n",
    "        pred_and_gt[k] = np.concatenate(v)\n",
    "    \n",
    "    \n",
    "    print('\\nEp.%05d - TRN: loss %.2f VAL: loss %.2f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "    print('\\nEp.%05d - TRN: acc %.2f VAL: acc %.2f' % (ep, avg_trn_acc, avg_val_acc), flush=True)\n",
    "        \n",
    "        \n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['trn_acc'].append(avg_trn_acc)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "    meta_info['val_acc'].append(avg_val_acc)\n",
    "    \n",
    "pkl.dump(meta_info, open(os.path.join('.', 'result.p'), 'wb'))\n",
    "saver.save(sess, \"./model/model\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b4cc74b278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_x\n",
      "target_y\n",
      "dropout_value\n",
      "fc/truncated_normal/shape\n",
      "fc/truncated_normal/mean\n",
      "fc/truncated_normal/stddev\n",
      "fc/truncated_normal/TruncatedNormal\n",
      "fc/truncated_normal/mul\n",
      "fc/truncated_normal\n",
      "fc/Variable\n",
      "fc/Variable/Assign\n",
      "fc/Variable/read\n",
      "fc/Const\n",
      "fc/Variable_1\n",
      "fc/Variable_1/Assign\n",
      "fc/Variable_1/read\n",
      "fc/MatMul\n",
      "fc/add\n",
      "LeakyRelu/alpha\n",
      "LeakyRelu/mul\n",
      "LeakyRelu/Maximum\n",
      "fc_1/truncated_normal/shape\n",
      "fc_1/truncated_normal/mean\n",
      "fc_1/truncated_normal/stddev\n",
      "fc_1/truncated_normal/TruncatedNormal\n",
      "fc_1/truncated_normal/mul\n",
      "fc_1/truncated_normal\n",
      "fc_1/Variable\n",
      "fc_1/Variable/Assign\n",
      "fc_1/Variable/read\n",
      "fc_1/Const\n",
      "fc_1/Variable_1\n",
      "fc_1/Variable_1/Assign\n",
      "fc_1/Variable_1/read\n",
      "fc_1/sub/x\n",
      "fc_1/sub\n",
      "fc_1/dropout/Shape\n",
      "fc_1/dropout/random_uniform/min\n",
      "fc_1/dropout/random_uniform/max\n",
      "fc_1/dropout/random_uniform/RandomUniform\n",
      "fc_1/dropout/random_uniform/sub\n",
      "fc_1/dropout/random_uniform/mul\n",
      "fc_1/dropout/random_uniform\n",
      "fc_1/dropout/add\n",
      "fc_1/dropout/Floor\n",
      "fc_1/dropout/div\n",
      "fc_1/dropout/mul\n",
      "fc_1/MatMul\n",
      "fc_1/add\n",
      "LeakyRelu_1/alpha\n",
      "LeakyRelu_1/mul\n",
      "LeakyRelu_1/Maximum\n",
      "fc_2/truncated_normal/shape\n",
      "fc_2/truncated_normal/mean\n",
      "fc_2/truncated_normal/stddev\n",
      "fc_2/truncated_normal/TruncatedNormal\n",
      "fc_2/truncated_normal/mul\n",
      "fc_2/truncated_normal\n",
      "fc_2/Variable\n",
      "fc_2/Variable/Assign\n",
      "fc_2/Variable/read\n",
      "fc_2/Const\n",
      "fc_2/Variable_1\n",
      "fc_2/Variable_1/Assign\n",
      "fc_2/Variable_1/read\n",
      "fc_2/sub/x\n",
      "fc_2/sub\n",
      "fc_2/dropout/Shape\n",
      "fc_2/dropout/random_uniform/min\n",
      "fc_2/dropout/random_uniform/max\n",
      "fc_2/dropout/random_uniform/RandomUniform\n",
      "fc_2/dropout/random_uniform/sub\n",
      "fc_2/dropout/random_uniform/mul\n",
      "fc_2/dropout/random_uniform\n",
      "fc_2/dropout/add\n",
      "fc_2/dropout/Floor\n",
      "fc_2/dropout/div\n",
      "fc_2/dropout/mul\n",
      "fc_2/MatMul\n",
      "fc_2/add\n",
      "Sigmoid\n",
      "wd_term/L2Loss\n",
      "wd_term/L2Loss_1\n",
      "wd_term/L2Loss_2\n",
      "wd_term/L2Loss_3\n",
      "wd_term/L2Loss_4\n",
      "wd_term/L2Loss_5\n",
      "wd_term/stack\n",
      "wd_term/mul/x\n",
      "wd_term/mul\n",
      "crit_loss/logistic_loss/zeros_like\n",
      "crit_loss/logistic_loss/GreaterEqual\n",
      "crit_loss/logistic_loss/Select\n",
      "crit_loss/logistic_loss/Neg\n",
      "crit_loss/logistic_loss/Select_1\n",
      "crit_loss/logistic_loss/mul\n",
      "crit_loss/logistic_loss/sub\n",
      "crit_loss/logistic_loss/Exp\n",
      "crit_loss/logistic_loss/Log1p\n",
      "crit_loss/logistic_loss\n",
      "crit_loss/Const\n",
      "crit_loss/Mean\n",
      "crit_loss/Const_1\n",
      "crit_loss/Sum\n",
      "crit_loss/add\n",
      "Cross_Entropy_Loss/tags\n",
      "Cross_Entropy_Loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/crit_loss/add_grad/Shape\n",
      "gradients/crit_loss/add_grad/Shape_1\n",
      "gradients/crit_loss/add_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/add_grad/Sum\n",
      "gradients/crit_loss/add_grad/Reshape\n",
      "gradients/crit_loss/add_grad/Sum_1\n",
      "gradients/crit_loss/add_grad/Reshape_1\n",
      "gradients/crit_loss/add_grad/tuple/group_deps\n",
      "gradients/crit_loss/add_grad/tuple/control_dependency\n",
      "gradients/crit_loss/add_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/Mean_grad/Reshape/shape\n",
      "gradients/crit_loss/Mean_grad/Reshape\n",
      "gradients/crit_loss/Mean_grad/Shape\n",
      "gradients/crit_loss/Mean_grad/Tile\n",
      "gradients/crit_loss/Mean_grad/Shape_1\n",
      "gradients/crit_loss/Mean_grad/Shape_2\n",
      "gradients/crit_loss/Mean_grad/Const\n",
      "gradients/crit_loss/Mean_grad/Prod\n",
      "gradients/crit_loss/Mean_grad/Const_1\n",
      "gradients/crit_loss/Mean_grad/Prod_1\n",
      "gradients/crit_loss/Mean_grad/Maximum/y\n",
      "gradients/crit_loss/Mean_grad/Maximum\n",
      "gradients/crit_loss/Mean_grad/floordiv\n",
      "gradients/crit_loss/Mean_grad/Cast\n",
      "gradients/crit_loss/Mean_grad/truediv\n",
      "gradients/crit_loss/Sum_grad/Reshape/shape\n",
      "gradients/crit_loss/Sum_grad/Reshape\n",
      "gradients/crit_loss/Sum_grad/Tile/multiples\n",
      "gradients/crit_loss/Sum_grad/Tile\n",
      "gradients/crit_loss/logistic_loss_grad/Shape\n",
      "gradients/crit_loss/logistic_loss_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss_grad/Sum\n",
      "gradients/crit_loss/logistic_loss_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss_grad/tuple/control_dependency_1\n",
      "gradients/wd_term/mul_grad/Shape\n",
      "gradients/wd_term/mul_grad/Shape_1\n",
      "gradients/wd_term/mul_grad/BroadcastGradientArgs\n",
      "gradients/wd_term/mul_grad/mul\n",
      "gradients/wd_term/mul_grad/Sum\n",
      "gradients/wd_term/mul_grad/Reshape\n",
      "gradients/wd_term/mul_grad/mul_1\n",
      "gradients/wd_term/mul_grad/Sum_1\n",
      "gradients/wd_term/mul_grad/Reshape_1\n",
      "gradients/wd_term/mul_grad/tuple/group_deps\n",
      "gradients/wd_term/mul_grad/tuple/control_dependency\n",
      "gradients/wd_term/mul_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Shape\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Sum\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Neg\n",
      "gradients/crit_loss/logistic_loss/sub_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/sub_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/add/x\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/add\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/Reciprocal\n",
      "gradients/crit_loss/logistic_loss/Log1p_grad/mul\n",
      "gradients/wd_term/stack_grad/unstack\n",
      "gradients/wd_term/stack_grad/tuple/group_deps\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_1\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_2\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_3\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_4\n",
      "gradients/wd_term/stack_grad/tuple/control_dependency_5\n",
      "gradients/crit_loss/logistic_loss/Select_grad/zeros_like\n",
      "gradients/crit_loss/logistic_loss/Select_grad/Select\n",
      "gradients/crit_loss/logistic_loss/Select_grad/Select_1\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/Select_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Shape\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Shape_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/BroadcastGradientArgs\n",
      "gradients/crit_loss/logistic_loss/mul_grad/mul\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Sum\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Reshape\n",
      "gradients/crit_loss/logistic_loss/mul_grad/mul_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Sum_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/Reshape_1\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/mul_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Exp_grad/mul\n",
      "gradients/wd_term/L2Loss_grad/mul\n",
      "gradients/wd_term/L2Loss_1_grad/mul\n",
      "gradients/wd_term/L2Loss_2_grad/mul\n",
      "gradients/wd_term/L2Loss_3_grad/mul\n",
      "gradients/wd_term/L2Loss_4_grad/mul\n",
      "gradients/wd_term/L2Loss_5_grad/mul\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/zeros_like\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/Select\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/Select_1\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/group_deps\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/control_dependency\n",
      "gradients/crit_loss/logistic_loss/Select_1_grad/tuple/control_dependency_1\n",
      "gradients/crit_loss/logistic_loss/Neg_grad/Neg\n",
      "gradients/AddN\n",
      "gradients/Sigmoid_grad/SigmoidGrad\n",
      "gradients/fc_2/add_grad/Shape\n",
      "gradients/fc_2/add_grad/Shape_1\n",
      "gradients/fc_2/add_grad/BroadcastGradientArgs\n",
      "gradients/fc_2/add_grad/Sum\n",
      "gradients/fc_2/add_grad/Reshape\n",
      "gradients/fc_2/add_grad/Sum_1\n",
      "gradients/fc_2/add_grad/Reshape_1\n",
      "gradients/fc_2/add_grad/tuple/group_deps\n",
      "gradients/fc_2/add_grad/tuple/control_dependency\n",
      "gradients/fc_2/add_grad/tuple/control_dependency_1\n",
      "gradients/fc_2/MatMul_grad/MatMul\n",
      "gradients/fc_2/MatMul_grad/MatMul_1\n",
      "gradients/fc_2/MatMul_grad/tuple/group_deps\n",
      "gradients/fc_2/MatMul_grad/tuple/control_dependency\n",
      "gradients/fc_2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_1\n",
      "gradients/fc_2/dropout/mul_grad/Shape\n",
      "gradients/fc_2/dropout/mul_grad/Shape_1\n",
      "gradients/fc_2/dropout/mul_grad/BroadcastGradientArgs\n",
      "gradients/fc_2/dropout/mul_grad/mul\n",
      "gradients/fc_2/dropout/mul_grad/Sum\n",
      "gradients/fc_2/dropout/mul_grad/Reshape\n",
      "gradients/fc_2/dropout/mul_grad/mul_1\n",
      "gradients/fc_2/dropout/mul_grad/Sum_1\n",
      "gradients/fc_2/dropout/mul_grad/Reshape_1\n",
      "gradients/fc_2/dropout/mul_grad/tuple/group_deps\n",
      "gradients/fc_2/dropout/mul_grad/tuple/control_dependency\n",
      "gradients/fc_2/dropout/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_2\n",
      "gradients/fc_2/dropout/div_grad/Shape\n",
      "gradients/fc_2/dropout/div_grad/Shape_1\n",
      "gradients/fc_2/dropout/div_grad/BroadcastGradientArgs\n",
      "gradients/fc_2/dropout/div_grad/RealDiv\n",
      "gradients/fc_2/dropout/div_grad/Sum\n",
      "gradients/fc_2/dropout/div_grad/Reshape\n",
      "gradients/fc_2/dropout/div_grad/Neg\n",
      "gradients/fc_2/dropout/div_grad/RealDiv_1\n",
      "gradients/fc_2/dropout/div_grad/RealDiv_2\n",
      "gradients/fc_2/dropout/div_grad/mul\n",
      "gradients/fc_2/dropout/div_grad/Sum_1\n",
      "gradients/fc_2/dropout/div_grad/Reshape_1\n",
      "gradients/fc_2/dropout/div_grad/tuple/group_deps\n",
      "gradients/fc_2/dropout/div_grad/tuple/control_dependency\n",
      "gradients/fc_2/dropout/div_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu_1/Maximum_grad/Shape\n",
      "gradients/LeakyRelu_1/Maximum_grad/Shape_1\n",
      "gradients/LeakyRelu_1/Maximum_grad/Shape_2\n",
      "gradients/LeakyRelu_1/Maximum_grad/zeros/Const\n",
      "gradients/LeakyRelu_1/Maximum_grad/zeros\n",
      "gradients/LeakyRelu_1/Maximum_grad/GreaterEqual\n",
      "gradients/LeakyRelu_1/Maximum_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu_1/Maximum_grad/Select\n",
      "gradients/LeakyRelu_1/Maximum_grad/Select_1\n",
      "gradients/LeakyRelu_1/Maximum_grad/Sum\n",
      "gradients/LeakyRelu_1/Maximum_grad/Reshape\n",
      "gradients/LeakyRelu_1/Maximum_grad/Sum_1\n",
      "gradients/LeakyRelu_1/Maximum_grad/Reshape_1\n",
      "gradients/LeakyRelu_1/Maximum_grad/tuple/group_deps\n",
      "gradients/LeakyRelu_1/Maximum_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu_1/Maximum_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu_1/mul_grad/Shape\n",
      "gradients/LeakyRelu_1/mul_grad/Shape_1\n",
      "gradients/LeakyRelu_1/mul_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu_1/mul_grad/mul\n",
      "gradients/LeakyRelu_1/mul_grad/Sum\n",
      "gradients/LeakyRelu_1/mul_grad/Reshape\n",
      "gradients/LeakyRelu_1/mul_grad/mul_1\n",
      "gradients/LeakyRelu_1/mul_grad/Sum_1\n",
      "gradients/LeakyRelu_1/mul_grad/Reshape_1\n",
      "gradients/LeakyRelu_1/mul_grad/tuple/group_deps\n",
      "gradients/LeakyRelu_1/mul_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu_1/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_3\n",
      "gradients/fc_1/add_grad/Shape\n",
      "gradients/fc_1/add_grad/Shape_1\n",
      "gradients/fc_1/add_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/add_grad/Sum\n",
      "gradients/fc_1/add_grad/Reshape\n",
      "gradients/fc_1/add_grad/Sum_1\n",
      "gradients/fc_1/add_grad/Reshape_1\n",
      "gradients/fc_1/add_grad/tuple/group_deps\n",
      "gradients/fc_1/add_grad/tuple/control_dependency\n",
      "gradients/fc_1/add_grad/tuple/control_dependency_1\n",
      "gradients/fc_1/MatMul_grad/MatMul\n",
      "gradients/fc_1/MatMul_grad/MatMul_1\n",
      "gradients/fc_1/MatMul_grad/tuple/group_deps\n",
      "gradients/fc_1/MatMul_grad/tuple/control_dependency\n",
      "gradients/fc_1/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_4\n",
      "gradients/fc_1/dropout/mul_grad/Shape\n",
      "gradients/fc_1/dropout/mul_grad/Shape_1\n",
      "gradients/fc_1/dropout/mul_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/dropout/mul_grad/mul\n",
      "gradients/fc_1/dropout/mul_grad/Sum\n",
      "gradients/fc_1/dropout/mul_grad/Reshape\n",
      "gradients/fc_1/dropout/mul_grad/mul_1\n",
      "gradients/fc_1/dropout/mul_grad/Sum_1\n",
      "gradients/fc_1/dropout/mul_grad/Reshape_1\n",
      "gradients/fc_1/dropout/mul_grad/tuple/group_deps\n",
      "gradients/fc_1/dropout/mul_grad/tuple/control_dependency\n",
      "gradients/fc_1/dropout/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_5\n",
      "gradients/fc_1/dropout/div_grad/Shape\n",
      "gradients/fc_1/dropout/div_grad/Shape_1\n",
      "gradients/fc_1/dropout/div_grad/BroadcastGradientArgs\n",
      "gradients/fc_1/dropout/div_grad/RealDiv\n",
      "gradients/fc_1/dropout/div_grad/Sum\n",
      "gradients/fc_1/dropout/div_grad/Reshape\n",
      "gradients/fc_1/dropout/div_grad/Neg\n",
      "gradients/fc_1/dropout/div_grad/RealDiv_1\n",
      "gradients/fc_1/dropout/div_grad/RealDiv_2\n",
      "gradients/fc_1/dropout/div_grad/mul\n",
      "gradients/fc_1/dropout/div_grad/Sum_1\n",
      "gradients/fc_1/dropout/div_grad/Reshape_1\n",
      "gradients/fc_1/dropout/div_grad/tuple/group_deps\n",
      "gradients/fc_1/dropout/div_grad/tuple/control_dependency\n",
      "gradients/fc_1/dropout/div_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu/Maximum_grad/Shape\n",
      "gradients/LeakyRelu/Maximum_grad/Shape_1\n",
      "gradients/LeakyRelu/Maximum_grad/Shape_2\n",
      "gradients/LeakyRelu/Maximum_grad/zeros/Const\n",
      "gradients/LeakyRelu/Maximum_grad/zeros\n",
      "gradients/LeakyRelu/Maximum_grad/GreaterEqual\n",
      "gradients/LeakyRelu/Maximum_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu/Maximum_grad/Select\n",
      "gradients/LeakyRelu/Maximum_grad/Select_1\n",
      "gradients/LeakyRelu/Maximum_grad/Sum\n",
      "gradients/LeakyRelu/Maximum_grad/Reshape\n",
      "gradients/LeakyRelu/Maximum_grad/Sum_1\n",
      "gradients/LeakyRelu/Maximum_grad/Reshape_1\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/group_deps\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu/Maximum_grad/tuple/control_dependency_1\n",
      "gradients/LeakyRelu/mul_grad/Shape\n",
      "gradients/LeakyRelu/mul_grad/Shape_1\n",
      "gradients/LeakyRelu/mul_grad/BroadcastGradientArgs\n",
      "gradients/LeakyRelu/mul_grad/mul\n",
      "gradients/LeakyRelu/mul_grad/Sum\n",
      "gradients/LeakyRelu/mul_grad/Reshape\n",
      "gradients/LeakyRelu/mul_grad/mul_1\n",
      "gradients/LeakyRelu/mul_grad/Sum_1\n",
      "gradients/LeakyRelu/mul_grad/Reshape_1\n",
      "gradients/LeakyRelu/mul_grad/tuple/group_deps\n",
      "gradients/LeakyRelu/mul_grad/tuple/control_dependency\n",
      "gradients/LeakyRelu/mul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_6\n",
      "gradients/fc/add_grad/Shape\n",
      "gradients/fc/add_grad/Shape_1\n",
      "gradients/fc/add_grad/BroadcastGradientArgs\n",
      "gradients/fc/add_grad/Sum\n",
      "gradients/fc/add_grad/Reshape\n",
      "gradients/fc/add_grad/Sum_1\n",
      "gradients/fc/add_grad/Reshape_1\n",
      "gradients/fc/add_grad/tuple/group_deps\n",
      "gradients/fc/add_grad/tuple/control_dependency\n",
      "gradients/fc/add_grad/tuple/control_dependency_1\n",
      "gradients/fc/MatMul_grad/MatMul\n",
      "gradients/fc/MatMul_grad/MatMul_1\n",
      "gradients/fc/MatMul_grad/tuple/group_deps\n",
      "gradients/fc/MatMul_grad/tuple/control_dependency\n",
      "gradients/fc/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/AddN_7\n",
      "gradients/AddN_8\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "fc/Variable/Adam/Initializer/zeros\n",
      "fc/Variable/Adam\n",
      "fc/Variable/Adam/Assign\n",
      "fc/Variable/Adam/read\n",
      "fc/Variable/Adam_1/Initializer/zeros\n",
      "fc/Variable/Adam_1\n",
      "fc/Variable/Adam_1/Assign\n",
      "fc/Variable/Adam_1/read\n",
      "fc/Variable_1/Adam/Initializer/zeros\n",
      "fc/Variable_1/Adam\n",
      "fc/Variable_1/Adam/Assign\n",
      "fc/Variable_1/Adam/read\n",
      "fc/Variable_1/Adam_1/Initializer/zeros\n",
      "fc/Variable_1/Adam_1\n",
      "fc/Variable_1/Adam_1/Assign\n",
      "fc/Variable_1/Adam_1/read\n",
      "fc_1/Variable/Adam/Initializer/zeros\n",
      "fc_1/Variable/Adam\n",
      "fc_1/Variable/Adam/Assign\n",
      "fc_1/Variable/Adam/read\n",
      "fc_1/Variable/Adam_1/Initializer/zeros\n",
      "fc_1/Variable/Adam_1\n",
      "fc_1/Variable/Adam_1/Assign\n",
      "fc_1/Variable/Adam_1/read\n",
      "fc_1/Variable_1/Adam/Initializer/zeros\n",
      "fc_1/Variable_1/Adam\n",
      "fc_1/Variable_1/Adam/Assign\n",
      "fc_1/Variable_1/Adam/read\n",
      "fc_1/Variable_1/Adam_1/Initializer/zeros\n",
      "fc_1/Variable_1/Adam_1\n",
      "fc_1/Variable_1/Adam_1/Assign\n",
      "fc_1/Variable_1/Adam_1/read\n",
      "fc_2/Variable/Adam/Initializer/zeros\n",
      "fc_2/Variable/Adam\n",
      "fc_2/Variable/Adam/Assign\n",
      "fc_2/Variable/Adam/read\n",
      "fc_2/Variable/Adam_1/Initializer/zeros\n",
      "fc_2/Variable/Adam_1\n",
      "fc_2/Variable/Adam_1/Assign\n",
      "fc_2/Variable/Adam_1/read\n",
      "fc_2/Variable_1/Adam/Initializer/zeros\n",
      "fc_2/Variable_1/Adam\n",
      "fc_2/Variable_1/Adam/Assign\n",
      "fc_2/Variable_1/Adam/read\n",
      "fc_2/Variable_1/Adam_1/Initializer/zeros\n",
      "fc_2/Variable_1/Adam_1\n",
      "fc_2/Variable_1/Adam_1/Assign\n",
      "fc_2/Variable_1/Adam_1/read\n",
      "Adam/learning_rate\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_fc/Variable/ApplyAdam\n",
      "Adam/update_fc/Variable_1/ApplyAdam\n",
      "Adam/update_fc_1/Variable/ApplyAdam\n",
      "Adam/update_fc_1/Variable_1/ApplyAdam\n",
      "Adam/update_fc_2/Variable/ApplyAdam\n",
      "Adam/update_fc_2/Variable_1/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "Round\n",
      "Equal\n",
      "Cast\n",
      "Const\n",
      "Mean\n",
      "Accuracy/tags\n",
      "Accuracy\n",
      "dataset/tensors/component_0\n",
      "dataset/tensors/component_1\n",
      "dataset/buffer_size\n",
      "dataset/seed\n",
      "dataset/seed2\n",
      "Iterator\n",
      "TensorSliceDataset\n",
      "ShuffleDataset\n",
      "BatchDataset/batch_size\n",
      "BatchDataset\n",
      "MakeIterator\n",
      "dataset_1/tensors/component_0\n",
      "dataset_1/tensors/component_1\n",
      "Iterator_1\n",
      "TensorSliceDataset_1\n",
      "BatchDataset_1/batch_size\n",
      "BatchDataset_1\n",
      "MakeIterator_1\n",
      "Merge/MergeSummary\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/RestoreV2_12/tensor_names\n",
      "save/RestoreV2_12/shape_and_slices\n",
      "save/RestoreV2_12\n",
      "save/Assign_12\n",
      "save/RestoreV2_13/tensor_names\n",
      "save/RestoreV2_13/shape_and_slices\n",
      "save/RestoreV2_13\n",
      "save/Assign_13\n",
      "save/RestoreV2_14/tensor_names\n",
      "save/RestoreV2_14/shape_and_slices\n",
      "save/RestoreV2_14\n",
      "save/Assign_14\n",
      "save/RestoreV2_15/tensor_names\n",
      "save/RestoreV2_15/shape_and_slices\n",
      "save/RestoreV2_15\n",
      "save/Assign_15\n",
      "save/RestoreV2_16/tensor_names\n",
      "save/RestoreV2_16/shape_and_slices\n",
      "save/RestoreV2_16\n",
      "save/Assign_16\n",
      "save/RestoreV2_17/tensor_names\n",
      "save/RestoreV2_17/shape_and_slices\n",
      "save/RestoreV2_17\n",
      "save/Assign_17\n",
      "save/RestoreV2_18/tensor_names\n",
      "save/RestoreV2_18/shape_and_slices\n",
      "save/RestoreV2_18\n",
      "save/Assign_18\n",
      "save/RestoreV2_19/tensor_names\n",
      "save/RestoreV2_19/shape_and_slices\n",
      "save/RestoreV2_19\n",
      "save/Assign_19\n",
      "save/restore_all\n",
      "init\n",
      "IteratorGetNext\n",
      "IteratorGetNext_1\n",
      "IteratorGetNext_2\n",
      "IteratorGetNext_3\n",
      "IteratorGetNext_4\n",
      "IteratorGetNext_5\n",
      "IteratorGetNext_6\n",
      "IteratorGetNext_7\n",
      "IteratorGetNext_8\n",
      "IteratorGetNext_9\n",
      "IteratorGetNext_10\n",
      "IteratorGetNext_11\n",
      "IteratorGetNext_12\n",
      "IteratorGetNext_13\n",
      "IteratorGetNext_14\n",
      "IteratorGetNext_15\n",
      "IteratorGetNext_16\n",
      "IteratorGetNext_17\n",
      "IteratorGetNext_18\n",
      "IteratorGetNext_19\n",
      "IteratorGetNext_20\n",
      "IteratorGetNext_21\n",
      "IteratorGetNext_22\n",
      "IteratorGetNext_23\n",
      "IteratorGetNext_24\n",
      "IteratorGetNext_25\n",
      "IteratorGetNext_26\n",
      "IteratorGetNext_27\n",
      "IteratorGetNext_28\n",
      "IteratorGetNext_29\n",
      "IteratorGetNext_30\n",
      "IteratorGetNext_31\n",
      "IteratorGetNext_32\n",
      "IteratorGetNext_33\n",
      "IteratorGetNext_34\n",
      "IteratorGetNext_35\n",
      "IteratorGetNext_36\n",
      "IteratorGetNext_37\n",
      "IteratorGetNext_38\n",
      "IteratorGetNext_39\n",
      "IteratorGetNext_40\n",
      "IteratorGetNext_41\n",
      "IteratorGetNext_42\n",
      "IteratorGetNext_43\n",
      "IteratorGetNext_44\n",
      "IteratorGetNext_45\n",
      "IteratorGetNext_46\n",
      "IteratorGetNext_47\n",
      "IteratorGetNext_48\n",
      "IteratorGetNext_49\n",
      "IteratorGetNext_50\n",
      "IteratorGetNext_51\n",
      "IteratorGetNext_52\n",
      "IteratorGetNext_53\n",
      "IteratorGetNext_54\n",
      "IteratorGetNext_55\n",
      "IteratorGetNext_56\n",
      "IteratorGetNext_57\n",
      "IteratorGetNext_58\n",
      "IteratorGetNext_59\n",
      "IteratorGetNext_60\n",
      "IteratorGetNext_61\n",
      "IteratorGetNext_62\n",
      "IteratorGetNext_63\n",
      "IteratorGetNext_64\n",
      "IteratorGetNext_65\n",
      "IteratorGetNext_66\n",
      "IteratorGetNext_67\n",
      "IteratorGetNext_68\n",
      "IteratorGetNext_69\n",
      "IteratorGetNext_70\n",
      "IteratorGetNext_71\n",
      "IteratorGetNext_72\n",
      "IteratorGetNext_73\n",
      "IteratorGetNext_74\n",
      "IteratorGetNext_75\n",
      "IteratorGetNext_76\n",
      "IteratorGetNext_77\n",
      "IteratorGetNext_78\n",
      "IteratorGetNext_79\n",
      "IteratorGetNext_80\n",
      "IteratorGetNext_81\n",
      "IteratorGetNext_82\n",
      "IteratorGetNext_83\n",
      "IteratorGetNext_84\n",
      "IteratorGetNext_85\n",
      "IteratorGetNext_86\n",
      "IteratorGetNext_87\n",
      "IteratorGetNext_88\n",
      "IteratorGetNext_89\n",
      "IteratorGetNext_90\n",
      "IteratorGetNext_91\n",
      "IteratorGetNext_92\n",
      "IteratorGetNext_93\n",
      "IteratorGetNext_94\n",
      "IteratorGetNext_95\n",
      "IteratorGetNext_96\n",
      "IteratorGetNext_97\n",
      "IteratorGetNext_98\n",
      "IteratorGetNext_99\n",
      "IteratorGetNext_100\n",
      "IteratorGetNext_101\n",
      "IteratorGetNext_102\n",
      "IteratorGetNext_103\n",
      "IteratorGetNext_104\n",
      "IteratorGetNext_105\n",
      "IteratorGetNext_106\n",
      "IteratorGetNext_107\n",
      "IteratorGetNext_108\n",
      "IteratorGetNext_109\n",
      "IteratorGetNext_110\n",
      "IteratorGetNext_111\n",
      "IteratorGetNext_112\n",
      "IteratorGetNext_113\n",
      "IteratorGetNext_114\n",
      "IteratorGetNext_115\n",
      "IteratorGetNext_116\n",
      "IteratorGetNext_117\n",
      "IteratorGetNext_118\n",
      "IteratorGetNext_119\n",
      "IteratorGetNext_120\n",
      "IteratorGetNext_121\n",
      "IteratorGetNext_122\n",
      "IteratorGetNext_123\n",
      "IteratorGetNext_124\n",
      "IteratorGetNext_125\n",
      "IteratorGetNext_126\n",
      "IteratorGetNext_127\n",
      "IteratorGetNext_128\n",
      "IteratorGetNext_129\n",
      "IteratorGetNext_130\n",
      "IteratorGetNext_131\n",
      "IteratorGetNext_132\n",
      "IteratorGetNext_133\n",
      "IteratorGetNext_134\n",
      "IteratorGetNext_135\n",
      "IteratorGetNext_136\n",
      "IteratorGetNext_137\n",
      "IteratorGetNext_138\n",
      "IteratorGetNext_139\n",
      "IteratorGetNext_140\n",
      "IteratorGetNext_141\n",
      "IteratorGetNext_142\n",
      "IteratorGetNext_143\n",
      "IteratorGetNext_144\n",
      "IteratorGetNext_145\n",
      "IteratorGetNext_146\n",
      "IteratorGetNext_147\n",
      "IteratorGetNext_148\n",
      "IteratorGetNext_149\n",
      "IteratorGetNext_150\n",
      "IteratorGetNext_151\n",
      "IteratorGetNext_152\n",
      "IteratorGetNext_153\n",
      "IteratorGetNext_154\n",
      "IteratorGetNext_155\n",
      "IteratorGetNext_156\n",
      "IteratorGetNext_157\n",
      "IteratorGetNext_158\n",
      "IteratorGetNext_159\n",
      "IteratorGetNext_160\n",
      "IteratorGetNext_161\n",
      "IteratorGetNext_162\n",
      "IteratorGetNext_163\n",
      "IteratorGetNext_164\n",
      "IteratorGetNext_165\n",
      "IteratorGetNext_166\n",
      "IteratorGetNext_167\n",
      "IteratorGetNext_168\n",
      "IteratorGetNext_169\n",
      "IteratorGetNext_170\n",
      "IteratorGetNext_171\n",
      "IteratorGetNext_172\n",
      "IteratorGetNext_173\n",
      "IteratorGetNext_174\n",
      "IteratorGetNext_175\n",
      "IteratorGetNext_176\n",
      "IteratorGetNext_177\n",
      "IteratorGetNext_178\n",
      "IteratorGetNext_179\n",
      "IteratorGetNext_180\n",
      "IteratorGetNext_181\n",
      "IteratorGetNext_182\n",
      "IteratorGetNext_183\n",
      "IteratorGetNext_184\n",
      "IteratorGetNext_185\n",
      "IteratorGetNext_186\n",
      "IteratorGetNext_187\n",
      "IteratorGetNext_188\n",
      "IteratorGetNext_189\n",
      "IteratorGetNext_190\n",
      "IteratorGetNext_191\n",
      "IteratorGetNext_192\n",
      "IteratorGetNext_193\n",
      "IteratorGetNext_194\n",
      "IteratorGetNext_195\n",
      "IteratorGetNext_196\n",
      "IteratorGetNext_197\n",
      "IteratorGetNext_198\n",
      "IteratorGetNext_199\n",
      "INFO:tensorflow:Restoring parameters from ./model/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2.8828258e-19]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Little test\n",
    "print(pws_1, pws_2)\n",
    "tst_x_matrix = w_s_network(num_node, pws_1, avg_degree)\n",
    "plt.imshow(tst_x_matrix, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "tst_x = tst_x_matrix.reshape(1, -1)\n",
    "truncated_tst_x = np.zeros((1, input_size))\n",
    "start = 0\n",
    "for j in range(num_node):\n",
    "    start = start + j\n",
    "    truncated_tst_x[0, start:start+j] = tst_x[0, j*num_node : j*num_node + j]\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "imported_graph = tf.train.import_meta_graph('./model/model.meta')\n",
    "\n",
    "for tensor in tf.get_default_graph().get_operations():\n",
    "    print (tensor.name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    imported_graph.restore(sess, './model/model')\n",
    "    yhat = sess.run(['Sigmoid:0'], feed_dict={'input_x:0':truncated_tst_x, 'dropout_value:0':0.0})\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
